{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>How can computers help us in our fight against Climate Change?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to explore the question above, combining a few of my passions and seeing how I could contribute to solving this issue. I plan on creating a Machine Learning algorithm, employing a Convoluted Neural Network, to accurately detect and sort out recyclable waste from other junk. This project is going to have 3 key aspects to it:  \n",
    "\n",
    "1. Collecting images to train\n",
    "\n",
    "2. Creating a model to accurately detect recyclable waste  \n",
    "\n",
    "3. Adding a live camera feed component, to show in real time whether the object is recyclable or not.\n",
    "\n",
    "For now, the scope is limited to only soda cans, due to time restrictions, but I have plans to expand it to support paper products and plastic bottles. I also plan on adding in my experience with robotics to make a physical device to sort automatically, so students can simply place the object in front of the camera and the machine will properly dispose of the waste for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Inspiration\n",
    "\n",
    "Hong Kong in general doesn't have the greates track record with the environment. From being notorious for bad air quality, to having one of the lowest recycling rates in the world despite being so developed, we as a city should be on the forefront in the fight against climate change. But, quite the opposite is true, and I think we must all push to change that narrative.  \n",
    "\n",
    "On a more personal note, since a young age, I've been passionate about the environment. I was an avid member of the sustainability club at my school since 6th grade, did my BSA Eagle Project in 8th grade focused on reducing plastic waste in our community, and have led out in my school's participation in the Hong Kong 10 Tonne Challenge. Further, I've co-founded a club in my school to tackle the issue of e-waste in our community. The environment is something I'm extremely passionate about, and I wanted to see how I could to my bit to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this Report  \n",
    "\n",
    "Having completed the first aspect of the project, I wanted to document my process, track my progress, and have a representation of all the work I put into this. This report goes in-depth into what I was looking for in my data, the various problems I faced and how I overcame them in sourcing my data, as well as how these images will fit into the next steps of my project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Outline\n",
    "\n",
    "After this introduction, this report goes into how I:  \n",
    "\n",
    "1. Established parameters and guidelines for my dataset\n",
    "\n",
    "* Requirements that my dataset needed to meet\n",
    "* Methodology of sourcing the data\n",
    "* How I will work with this data\n",
    "* How I will prepare this data to be optimal for my use  \n",
    "\n",
    "2. Scraped various websites for my data  \n",
    "3. Evaluated the strenghts and weaknesses of the images I had\n",
    "4. Plan on deploying this data into my machine learning algorithm  \n",
    "\n",
    "I also include additional thoughts at the end on how I think my data currently fulfills my needs, and what challenges I expect to encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "When I first started thinking about the data I would need, I didn't quite know what I was getting myself into. I'm still relatively new to python, and I've never done anything with web scraping or image downloading before. Further, I initially saw various pre-built tools online, such as a Google Images scraper. However, a lot of websites continue to make it harder to scrape data off of them, and as a result, I really didn't realize what a challenge getting my images was going to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements of Dataset\n",
    "\n",
    "I had done a few machine learning projects prior to this. I'd made a model to predict the direction of a stock, given historical data, along with detect cells with cancer. Even though the dataset had been provided for both of them, it gave me valuable insight into exactly what kind of data an algorithm needs to make it effective. After revisiting my previous projects I came up with the following criteria for the dataset I was putting together:  \n",
    "\n",
    "1. **Sufficient Quantity:** I wanted to ensure that I had enough data to actually train my model. With the cancer cell project, I had almost 2000 images, and as a result, I wanted to make sure I had a similar number of images at the minimum. \n",
    " \n",
    "2. **Detail Rich:** One of the hallmarks of a Convolutional Neural Network (referred to as CNN or ConvNet in this document) is it's ability to detect individual features amongst a wide array of images. However, the features cannot be detected if they don't exist in the first place. I wanted to ensure that the images I was getting were actually helpful in training the network, and weren't just random images of cans.  \n",
    "\n",
    "3. **Highly Versatile:** If my end goal is to be able to attach a live camera feed and detect real life cans, I thought that my dataset would really need to cover a large variety of color conditions, lighting conditions, etc. I don't know what equipment I will end up working with, and I don't know where I would set it up. I wanted to make sure that my model would be accurate regardless of the environment it is implemented in, and to achieve that, I had to have a diverse dataset.  \n",
    "\n",
    "With these parameters established, I had a good set of goals to meet, and was able to focus on images that fulfilled at least one of these goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Goal 2:\n",
    "\n",
    "![\"text\"](29.jpg)\n",
    "\n",
    "This image is perfect for my neural network, is it explicitly shows one of the key features of a soda can - the tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Goal 3:\n",
    "\n",
    "![text](Flickr150.jpg)\n",
    "\n",
    "The image above, although not a great image on it's own, actually serves an extremely valuable purpose for my project. The lighting conditions are quite dark, the colors are undersaturated, and all the other things that make it a not-so-great photo, will go to make my model stronger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sourcing the Dataset  \n",
    "\n",
    "As mentioned earlier, I thought that collecting my data would be as easy as providing my search query into a tool created by someone else, and that a lot of my programming would be focused on developing, tuning, and refining my machine learning model. However, this was most certainly not the case.  \n",
    "\n",
    "In order to get my data, I had to create various web scrapers, sifting through HTML pages of various sources, and finding the image link to download.  \n",
    "\n",
    "As for the sources themselves, I picked three: CAN Stock Images, iStock Images, and Flickr Photos. I go much more in depth on why I picked these three, as well as my process of scraping for the pictures and downloading them in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the Dataset\n",
    "\n",
    "Once I have the images downloaded, working with it would be no problem at all. I have had experience working with images and converting them and storing them in tensors for easy implementation with machine learning models.  \n",
    "\n",
    "All digital images can be represented with a few different values determining each pixel. When these are put together, they create a specific signal that your computer uses to display it. Our process would just be undoing this, and getting to those raw numbers so we can feed them into the neural network and train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing my Dataset\n",
    "\n",
    "I also discuss this in a bit more detail later in this report. However, in order to be consistent with Goals 1 and 3, I will also be modifying my dataset before feeding it into my model. I will be applying filters, transformations, and other image manipulations onto each image, to make my dataset more vast, more versatile, and more equipped to accurately detect cans. These forms of image augmentation also help prevent overfitting to the data, and further increase model accuracy through that. This process will be done at a later stage in the project, when I'm actually putting together and making my neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this section, I will be discussing the process I took to actually obtain my dataset. After identifying what exactly I needed from my dataset, I had to select my sources. Upon a bit of consideration, I decided to scrape images off of three websites: CAN Stock, iStock, and Flickr. I chose the first two sites since the stock images would provide clean, well-lit, clear images with easily identifable features (i.e. the pull tab, the lip, the concave shape of the base). On the other hand, with Flickr being user-uploaded images, I thought they would provide a bit more of a \"real-life\" aspect to it - from natural lighting to crumpled cans.\n",
    "\n",
    "Then, I will also detail how I made my dataset of \"Not Cans\". For this, I ended up creating various random noise images and exporting them, to be easily accessible and for us to be able to see them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images of Soda Cans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages\n",
    "As with any program, the first step is to import all the packages we will be using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Package | Use  |\n",
    "| :---: | :--- |\n",
    "| time | We will be using the 'time' package in order to set timeouts for various loops, as well as give the pages that we visit time to load  |\n",
    "| selenium | The 'selenium' package will be used for the purpose of emulating a web browser. For my personal use case, I used Google Chrome since I already had it installed. A little prior setup is required, which I will detail in next steps, but the package itself is used to emulate and automate a web browser, run commands like scrolling, clicking, text input, as well as retrieve the HTML code from it  |\n",
    "| bs4 | 'bs4', better known as BeautifulSoup4, is a great package when dealing with HTML. It allows you to efficiently sort through the source code, extract various tags and properties of said tags, and other analysis of HTML code  |\n",
    "| urllib | 'urllib' is what I originally used for downloading images. It workes very smoothly for CAN and iStock images, however it does have it's limitations. It's great for downloading, since it is quite intuitive. But that does mean it has it's own limitations, and for Flickr I had to employ two other packages  |\n",
    "| requests | This was one of the packages I had to switch to in order to download images from flickr. With the way that I scraped these images, I had to navigate to the link of the image itself, and the way the 'urllib' does it is forbidden. As a result, I used 'requests' to access the images and download them  |\n",
    "| shutil | Last but not least, 'shutil' was used in conjunction with 'requests' for FLickr. This package is extremely useful for working with your file directory, and I used it to copy the image from the request into the directore.  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Code Requirements\n",
    "import time\n",
    "\n",
    "# Automated Web Browser and HTML Manipulation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Downloading Images from CAN and iStock\n",
    "import urllib.request\n",
    "\n",
    "# Downloading Images from Flickr\n",
    "import requests\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the WebDriver\n",
    "\n",
    "Having imported all our packages, the next step is to initialize and create an instance of the automated web browser we will be using. First, we configure some of the settings of the driver. Setting the 'headless' value of options to be True means that it will run in the background, without rendering that browser itself. Although setting it to False was useful when I was debugging the program, setting it to True when actually running the full code allows it to be a bit faster and more resource-efficient.\n",
    "\n",
    "We must also specify the window size of the driver. Since my MacBook has a screen resolution of 1920 x 1200, I just set it as that, but the resolution itself shouldn't make a big difference, and can easily be adjusted to meet the resolution of your device.\n",
    "\n",
    "The next step is to create an instance of the WebDriver. First we must lead the WebDriver to the path of the driver. As I mentioned earlier, I personally am using Chrome, and as a result of that, had to install the chrome driver from the Chromium Open Source Project. The driver can be found here: https://chromedriver.chromium.org\n",
    "\n",
    "Lastly, we create a WebDriver instance, and assign it to the variable \"driver\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuring the preferences for our WebDriver\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# Locating the path for the driver, and creating a WebDriver object\n",
    "DRIVER_PATH = '/Users/anonymousvikram/chromedriver'\n",
    "driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the HTML and Image Links from each page\n",
    "\n",
    "Now that our WebDriver has been set up, we get to the fun part! Each site has a different process. However, CAN Stock and iStock are quite similar, just slight tweaks in the code. So I've split up this section to discuss the two together, and then Flickr seperately. Note that Flickr was a bit of a challenge due to the way they display images, and so the code for that is quite a bit more extensive and jumps through more hurdles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAN Stock Images and iStock Images  \n",
    "Firstly, I'll talk through the process of getting the HTML and seperating the image links for CAN Stock and iStock. In this step, there's only two lines that are different, but I'll make sure to mention it in my explanation as well as comment it out in the code below.  \n",
    "\n",
    "We begin by navigating to the web page on the driver we set up. This is the first point of difference. Since the two sites are obviously different, the URL you navigate to for each one is different. However, the command is still the same, and the next steps are also identical. Then, we run a while loop for 15 seconds, during which the driver just keeps scrolling down. This ensures that all the images on the first page load, so that we can extract their URLs and download them. It is certainly possible to navigate to further pages, but I thought that the data I got from this was sufficient.  \n",
    "\n",
    "After we scroll to the bottom of the page and the while loop has finished, we get the HTML code of the page by accessing the 'page_source' attribute of the driver, and storing that in a variable called 'source_code'. Then, we quit the driver to save resource, and print \"Completed Selenium\" to provide a status update for the user. This has now given us the HTML of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The next line is for scraping from CAN Stock Images\n",
    "driver.get(\"https://www.canstockphoto.com/images-photos/soda-cans.html\")\n",
    "\n",
    "# The next line is for scraping from iStock Images\n",
    "driver.get(\"https://www.istockphoto.com/hk/圖片/soda-cans?mediatype=photography&phrase=soda%20cans&sort=mostpopular\")\n",
    "\n",
    "# Scroll to the bottom of the page\n",
    "start = time.time()\n",
    "while time.time() <= start + 15:\n",
    "    driver.execute_script(\"window.scrollBy(0, 1000)\")\n",
    "\n",
    "# Store the HTML of the page and quit the driver\n",
    "source_code = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# Inform user of progress\n",
    "print(\"Completed Selenium\")\n",
    "\n",
    "# Create BeautifulSoup object from the HTML code\n",
    "soup = BeautifulSoup(source_code)\n",
    "\n",
    "# The next line is for scraping from CAN Stock Images\n",
    "images = soup.find_all(\"article\")\n",
    "\n",
    "# The next line is for scraping from iStock Images\n",
    "images = soup.find_all(\"a\", {\"class\": \"gallery-mosaic-asset__link\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the HTML on its own is not very helpful for our purpose. We have to sift through the code and locate the specific URLs that will allow us to download the images. That's where BeautifulSoup comes in. We create a new BeautifulSoup instace, passing it the 'source_code' we just extracted, and storing this BeautifulSoup instance in a variable called 'soup'. \n",
    "\n",
    "\n",
    "The final step is once again different between CAN Stock Images and iStock Images. The two display search results differently, and as a result the HTML code must be sorted in different ways. In the case of CAN Stock, we  use the 'BeautifulSoup.find_all()' method on 'soup', and search for all items that are classified as an \"article\". For iStock however, we use the same method on the same object, but instead search for all items that are classified as \"a\", and have their \"class\" identifier set as \"gallery-mosaic-asset__link\". \n",
    "\n",
    "This method returns an iterable object, of which each entry consists of a string of text with the general location of the URL. We will run further bs4 methods on this string in a later step, to pinpoint the image URL and download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flickr Images Part One - Accepting Cookies  \n",
    "Now, with Flickr Images, there were countless hurdles I had to jump through to scrape images off their website. This next code block tackles the first such challenge. As with the previous example, the first step is to navigate to the web page with the results. However, this is where we diverge a bit. On Flickr, upon visiting the website for the first time, you are presented with a cookies dialog, asking for permission to use cookies to enhance your web experience. However, sometimes the dialog is slow to appear. So, we do a few steps:  \n",
    "\n",
    "1. We first scroll down slightly, to make sure the dialog appears.  \n",
    "\n",
    "2. We *look* for the button to \"Accpet All Cookies\". The key step here is that we look to ensure the button is indeed there. Without the if loop, in the case the browser already has cookie settings saved, the code would return an exceptionError and stop immediately.  \n",
    "\n",
    "3. Once found, we inform the user that the button is indeed found, just as a bit of feedback in case running it in Headless mode.  \n",
    "\n",
    "4. Now that we know the button exists, we use the \"find_element_by_xpath()\" method on our driver, to locate the button. Notice the difference between the two commands. When looking for it, we use \"find_element**s**_by_xpath()\". This method returns a list of all instances, so if it returns an empty list, it won't return an exceptionError.  \n",
    "\n",
    "5. Lastly, we inform the user that the code has successfully found, and clicked to agree to all cookies.  \n",
    "\n",
    "After accepting cookies, Flickr actually reloads its page, so we make our code wait for 10 seconds to let that process take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the Web Page\n",
    "driver.get(\"https://www.flickr.com/search/?text=soda%20can&view_all=1\")\n",
    "\n",
    "# Timout to let the page load\n",
    "time.sleep(5)\n",
    "\n",
    "# Small scroll to ensure cookies dialog appears\n",
    "driver.execute_script(\"window.scrollBy(0, 100)\")\n",
    "\n",
    "# Look for Button to Accept Cookies, without interrupting the code in case no such button is found\n",
    "if(len(driver.find_elements_by_xpath(\"//button[@id='truste-consent-button']\")) != 0):\n",
    "\n",
    "    # Update the user on progress\n",
    "    print(\"Found button\")\n",
    "\n",
    "    # Navigate to the Accept Button and Click it\n",
    "    driver.find_element_by_xpath(\"//button[@id='truste-consent-button']\").click()\n",
    "\n",
    "    # Update the user on progress\n",
    "    print(\"Agreed\")\n",
    "\n",
    "# Small timeout to let the page refresh after accepting cookies\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flickr Images Part Two - Scrolling, Clicking, and Much More  \n",
    "\n",
    "This next part is significantly more complicated than what I had to do to for the other two sources. There's a variety of factors that makes scraping images off of Flickr challenging. \n",
    "\n",
    "The first is that results are displayed in an infinite scroll, rather than over multiple pages. This means that scraping is no longer just scroll to the bottom of the page, save HTML, and go to page 2. Further, when you scroll far enough, Flickr actually starts unloading the images near the top to keep your experience smooth. Although great for user experience, when automating, it turns out to be a real challenge for getting each image URL. Lastly, althought Flickr uses an \"infinite scroll\", after a while it starts making you clikc a \"Load More Results\" button, and so we had to implement a solution for that as well.  \n",
    "\n",
    "It's clear that all of these problems are quite critical to the success of the scraper and each one must be dealt with. For the issue of the infinite scroll, I solved this by implementing a few novel procedures. First, I would extract the HTML of the page every 5 scrolls, rather than after scrolling to the very bottom. This deals with the issue of images at the top unloading. However, it also creates a new problem. It's extremely easy to start getting duplicate images with this method. So each time, I actually process the HTML immediately after, and extract the URL of the lead. Then, I store that URL in an array, so that it can be referred to later when actually downloading the images. But the array actually also serves a second purpose: I can use it to cross-reference already processed images and prevent duplicates! Last but not least, Flickr's \"Load More Results\" button actually changes between two possibilites. So I implement code similar to when accepting cookies, to check for the load more button, and if it is found, to click on the button.\n",
    "\n",
    "When all of this is put together, although my loop runs for longer, it quite efficiently processes and stores the image leads into an array, which can be easily iterated through, tracked down, and downloaded from.\n",
    "\n",
    "Although it seems like a lot of work, the sheer number of images I got from this process made it well worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing array for leads to be used later when downloading, as well as to prevent repeats\n",
    "imageLeads = []\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Update the user on progress\n",
    "print(\"starting\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Run the 'infinite' scroll for 5 minutes\n",
    "while time.time() <= start + 300:\n",
    "\n",
    "    # Scroll down the page\n",
    "    driver.execute_script(\"window.scrollBy(0, 12000)\")\n",
    "\n",
    "    # Update the user on progress\n",
    "    print(\"Scroll\")\n",
    "\n",
    "    # Short delay to allow results to load\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Clicking \"Load More\" button\n",
    "    if(len(driver.find_elements_by_xpath(\"//button[@class='alt']\")) != 0):\n",
    "            \n",
    "            driver.find_element_by_xpath(\"//button[@class='alt']\").click()\n",
    "            \n",
    "            # Update the user on progress\n",
    "            print(\"load more\")\n",
    "\n",
    "    # Clicking \"Load More\" button as well\n",
    "    if(len(driver.find_elements_by_xpath(\"//button[@class='alt no-outline']\")) != 0):\n",
    "        \n",
    "        driver.find_element_by_xpath(\"//button[@class='alt no-outline']\").click()\n",
    "        \n",
    "        # Update the user on progress\n",
    "        print(\"load more\")\n",
    "    \n",
    "    # Update the number of times scrolled\n",
    "    counter = counter + 1\n",
    "    \n",
    "    # Extract HTML and get URLs every 5 scrolls\n",
    "    if(counter == 5):\n",
    "\n",
    "        # Update the user on progress\n",
    "        print(\"Loading pictures now\")\n",
    "\n",
    "        # Delay to let results load\n",
    "        time.sleep(15)\n",
    "\n",
    "        # Update the user on progress\n",
    "        print(\"Done Loading\")\n",
    "\n",
    "        # Extracting HTML from the Webpage\n",
    "        plainText = driver.page_source\n",
    "\n",
    "        # Creating a BeautifulSoup object with the HTML extracted\n",
    "        soupTemp = BeautifulSoup(plainText)\n",
    "\n",
    "        # Find all links to images in the HTML and store it in an iterable object\n",
    "        imageIteration1 = soupTemp.find_all(\"a\", {\"class\": \"overlay\"})\n",
    "\n",
    "        # Iterate through the iterable to extract the URLs\n",
    "        for imageLead in imageIteration1:\n",
    "\n",
    "            # Find and Create the image URL   \n",
    "            href = imageLead.extract().get('href')\n",
    "            href = \"https://www.flickr.com\" + href\n",
    "\n",
    "            # Setting up a variable to determine if image has been found before\n",
    "            isRepeat = False\n",
    "\n",
    "            # Going through existing leads to see if image has been processed before\n",
    "            for usedLead in imageLeads:\n",
    "                if(usedLead == href):\n",
    "\n",
    "                    # Setting the variable to true so that image isn't processed again\n",
    "                    isRepeat = True\n",
    "            \n",
    "            if(isRepeat):\n",
    "                \n",
    "                # Update the user on progress\n",
    "                print(\"Repeat image lead\")\n",
    "            \n",
    "            else:\n",
    "\n",
    "                # Update the user on progress\n",
    "                print(href)\n",
    "                \n",
    "                # Adding the URL to the list to download later and prevent duplicates\n",
    "                imageLeads.append(href)\n",
    "        \n",
    "        # Update the user on progress\n",
    "        print(str(len(imageLeads)) + \" image leads found so far\")\n",
    "\n",
    "        # Resetting Counter\n",
    "        counter = 0\n",
    "\n",
    "        # Update the user on progress\n",
    "        print(str(time.time() - start) + \" seconds\")\n",
    "\n",
    "# Update the user on progress\n",
    "print(str(len(imageLeads)) + \" image leads found\")\n",
    "\n",
    "# Quitting Selenium Driver\n",
    "driver.quit()\n",
    "\n",
    "# Small delay to allow system to recover\n",
    "print(\"sleeping zzz\")\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flickr Images Part Three -  Getting Image URLs from Leads  \n",
    "\n",
    "You may have noticed that in my previous explanation, I referred to the end result being an array of 'leads', rather than direct image URLs. There is actually a very good reason for this. With Flickr, the URL you can extract leads you directly to the user's photo album, with a highlight on the photo you selected. Downloading this would just be downloading a web page, rather than an image. So it is actually necessary to navigate to the URL we extracted, and run BeautifulSoup and sort through the HTML all over again, to ultimately get the image URL that we can download.\n",
    "\n",
    "In order to implement this, I make a new Selenium instance using the same options as earlier, and iterate through all the leads we collected. We then look for all images, get their \"src\" property and select all the text from the third character onwards (the first two are '\\\\' and interfere with transforming the src into a standalone URL). We then add the \"https://\" at the beginning of this src to get the final image URL!. Then, we print it to update the user on the progress, store it into an array to be downloaded, and repeat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the user on progress\n",
    "print(\"Finding True Image URLs\")\n",
    "\n",
    "# Initializing empty array to store image urls\n",
    "trueImageUrls = []\n",
    "\n",
    "# Iteration variable to keep the user informed on progress\n",
    "tempInt = 0\n",
    "\n",
    "# Setting up new Selenium driver\n",
    "driverImg = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "\n",
    "# Iterating through leads found in previous step\n",
    "for href in imageLeads:\n",
    "\n",
    "    # Going to the URL of the lead\n",
    "    driverImg.get(href)\n",
    "\n",
    "    # Storing the HTML of the page\n",
    "    trueImageSource = driverImg.page_source\n",
    "\n",
    "    # Creating a BeautifulSoup instance with the HTML to sort through\n",
    "    trueImageSoup = BeautifulSoup(trueImageSource)\n",
    "\n",
    "    # Looking for all images in the code\n",
    "    trueImageLink = trueImageSoup.find(\"img\")\n",
    "\n",
    "    # Getting the src property of the image, and selecting all characters from the third one\n",
    "    trueHref = trueImageLink.get(\"src\")[2:]\n",
    "\n",
    "    # Finalizing image URL\n",
    "    trueHref = \"https://\" + trueHref\n",
    "\n",
    "    # Update the user on progress\n",
    "    print(str(tempInt) + \"/\" + str(len(imageLeads)) + \": \" + trueHref)\n",
    "    \n",
    "    # Storing image URL to be downloaded\n",
    "    trueImageUrls.append(trueHref)\n",
    "\n",
    "    # Increasing the progress iterator\n",
    "    tempInt = tempInt + 1\n",
    "print(str(len(trueImageUrls)) + \" true images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Images from the Image Links  \n",
    "\n",
    "This next section will cover how we download the images from the URLs we've extracted thus far. Again, CAN Stock and iStock were quite similar in this process, but Flickr needed it's own way, so I'll discuss both of the different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAN Stock Images and iStock Images  \n",
    "\n",
    "In this case, the code once again only differs by two. Downloading is made very straightforward with the usage of the urllib package. However, the differences come in narrowing down the actual url of the image, and saving it. Similar to last time, the tags that each website uses is slightly different, and so narrowing it down is changed up through that. Then, when saving images, I changed the names to differentiate between the two sources.\n",
    "\n",
    "For the download itself, we use the 'request.urlretrieve()' function, providing both the link to download from as well as the path of the file to download to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration Variable for naming file\n",
    "i = 0\n",
    "\n",
    "# Looping through all the images found in previous step\n",
    "for link in images:\n",
    "\n",
    "    # The next line is for downloading from CAN Stock Images\n",
    "    href = link.extract().find(\"a\").find(\"img\").get(\"src\")\n",
    "\n",
    "    # The next line is for downloading from iStock Images\n",
    "    href = link.extract().find(\"figure\").find(\"img\").get('src')\n",
    "\n",
    "    print(href)\n",
    "\n",
    "    # The next line is for downloading from CAN Stock Images\n",
    "    full_name = \"/Users/anonymousvikram/recyclingVision/downloads/drink can single/canStock\" + str(i) + \".jpg\"\n",
    "\n",
    "    # The next line is for downloading from iStock Image\n",
    "    full_name = \"/Users/anonymousvikram/recyclingVision/downloads/drink can single/iStock\" + str(i) + \".jpg\"\n",
    "\n",
    "    # Actually download the file\n",
    "    urllib.request.urlretrieve(href, full_name)\n",
    "    i += 1\n",
    "# Let the user know how many images were downloaded\n",
    "print(str(i) + \" images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flickr Download  \n",
    "\n",
    "Once again, Flickr was slightly more complicated, although altogether not too bad. I was unable to use urllib, since the way they access the image is forbidden by flickr. However, by hacking togther code from the 'requests' and 'shutil' packages, I was able to put together a workaround.\n",
    "\n",
    "Firstly, we define the location path of the image. After doing that, we provide a little update to the user on the progress. Then, we use requests to get the image as a raw file. We open up the file at the location determined, decode the content, and use shutil to copy the raw file into the distantion. We repeat this for all the URLs we extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up iteration variable for progress and naming\n",
    "tempInt = 0\n",
    "\n",
    "# Iterating through all URLs collected\n",
    "for imageUrl in trueImageUrls:\n",
    "\n",
    "    # Determining path of the file\n",
    "    full_name = \"/Users/anonymousvikram/recyclingVision/downloads/drink can single/Flickr\" + str(\n",
    "            tempInt) + \".jpg\"\n",
    "    \n",
    "    # Update the user on progress\n",
    "    print(str(tempInt) + \"/\" + str(len(trueImageUrls)) + \": \" + full_name[66:])\n",
    "\n",
    "    # Get the image file using requests\n",
    "    r = requests.get(imageUrl, stream=True)\n",
    "\n",
    "    # Open a file at the destination\n",
    "    with open(full_name, 'wb') as destination:\n",
    "        \n",
    "        # Decode the image file\n",
    "        r.raw.decode_content = True\n",
    "\n",
    "        # Copy the file into the directory\n",
    "        shutil.copyfileobj(r.raw, destination)    \n",
    "    \n",
    "    # Increment the iterator\n",
    "    tempInt += 1\n",
    "\n",
    "# Let the user know how many images were downloaded\n",
    "print(str(tempInt) + \" images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images of Not Soda Cans\n",
    "\n",
    "Now that we have our dataset of Soda Cans, we have to generate images that aren't soda cans. For the Machine Learning algorithm to train correctly, since we're using binary classification, it needs images of both labels. As a result, instead of scraping random images off the web, I decided to generate random noise images. These are created using random values in NumPy, converted into images, and saved so that we can visualize them and reuse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages  \n",
    "\n",
    "This endeaver is far simpler than the previous, and only requires two packages:\n",
    "\n",
    "| Package | Use |\n",
    "| :---: | :-- |\n",
    "| NumPy | NumPy is probably one of the most famous packages for Python. It is extremely powerful, and augments what you can do with the language. For our use case, we will be using it to generate a 3D array of random values, which we will then use to convert into an image |\n",
    "| Python Imaging Library | PIL, better known as Python Imaging Library, will be utilized in order to convert our images from random numbers to actual images. It will take the thress values, for RGBA, and then produce an image out of it. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Downloading Images\n",
    "\n",
    "Now that our work environment is all set up, we can get started! I wanted to have an approximate even split between pictures of cans and pictures of not cans, so I ran this code 200 times using the for loop. We begin by creading a 3D Numpy Array of random values. The 400 and 400 show that the image itself will be 400x400 pixels, and the 3 shows that each pixel has 3 valus: Red Green and Blue. We also multiply this array by 255, since the np.random.rand() function generates values between 0 and 1, and said values have to be scaled up to generate the colors we want.\n",
    "\n",
    "We then use PIL to generate the image from the array. We specify that each value is of type uint8, and that we want it to convert using the \"RGBA\" (Red Green Blue Alpha) Color scheme.\n",
    "\n",
    "Finally, we finish it off by saving it into the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a for loop to make 2000 images\n",
    "\n",
    "for i in range(2000):\n",
    "    \n",
    "    # Generating random values to make an image out of\n",
    "    imageArray = np.random.rand(400,400,3) * 255\n",
    "\n",
    "    # Converting random values into an actual image\n",
    "    image = Image.fromarray(imageArray.astype(\"uint8\")).convert(\"RGBA\")\n",
    "\n",
    "    # Saving that image into the directory\n",
    "    image.save(\"/Users/anonymousvikram/recyclingVision/downloads/not drink can single/Flickr\" + str(\n",
    "            i) + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Post-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "I think one of the greatest strengths of my dataset is the sheer size of it. I have over 4000 images, which I will likely split into 3000 for training and 1000 for testing. Further, I think that the variety within my dataset is also very beneficial. As I mentioned earlier, I have two sources of stock photos, which will have good lighting and coloring, and will show all the features very clearly. But I also have over 1500 images from Flickr, that will be a bit more realistic, with slightly imperfect cans, and unideal lighting and color conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings\n",
    "\n",
    "Although most of this was automated, one of the most tedious parts of this undertaking was sorting through the images themselves. Especially with Flickr images, they will often have cans in them but may not be appropriate for the algorithm to learn from, or may not be beneficial in any way for the ConvNet I plan on using (see example below). Sorting through over 2000 images and determining which ones would be good for my Neural Network was quite a tedious task, and I think with a bit of refinement on my search parameters, I could've avoided having to do it so meticulously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](Flickr846.jpg)\n",
    "\n",
    "The image above was in my original dataset, and showed up as a \"soda can\" on Flickr. However, it clearly isn't, and had to be manually removed from the dataset to prevent it from disrupting the accuracy of my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Considerations\n",
    "Going ahead with my project, I do have one concern about my dataset. For the images of not cans, I have an abundance of random noise images (see below). However, Since I'm using a ConvNet, my algorithm might end up learning the random noise to be not can. That in itself isn't too big of an issue, but my fear is that it'll start detecting *only* random noise as not cans, and that my algorithm might fail when presented with actual trash. If this is the case, I will likely have to use the various web scrapers I've build to repopulate a random image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![text](notCan80.png) | ![text](notCan2261.png) |\n",
    "| :---: | :---: |\n",
    "\n",
    "Above are two examples of the random noise images that were generated. It becomes very clear side by side how they look quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "On the flip side, I do have a powerful tool at my disposal when training my algorithm. I will likely augment my current dataset numerous times, applying various color filters, rotating, stretching, and performing other transformations on the image, etc. I think that in addition with the sheer number of images I have will enable me to have a strong, reliable dataset - one that will be effective in training my algorithm to detect cans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome of Dataset\n",
    "\n",
    "I think overall, I'm quite happy with the data I have now. In my opinion, I managed to meet all three goals I set out for my data, and went above in beyond in some of them as well. I never expected to have this many images, and am quite proud of what I was able to accompish in this first segment of the project.  \n",
    "\n",
    "\n",
    "The data I've compiled can be found at https://bit.ly/3l8Nmhn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges due to Data\n",
    "\n",
    "The biggest issue I see right now from the data I have is just the sizing. For the non-cans, I standardized the size to an even 400x400 square. However, for all the images I scraped, they all have different resolutions, and almost none of them are square. Scaling it down may result in too much distortion, meaning that I may not be able to apply some of the transformations I would like to. However, I think I have a few different ways of circumventing this, and will detail my findings in the next report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Information\n",
    "\n",
    "If you would like to inquire about anything detailed in this report or give suggestions/feedback, please reach out to me at 220602@hkis.edu.hk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Thank you for reading my report. </center> </h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
